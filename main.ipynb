{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gret_datad(folders):\n",
    "    faces = []\n",
    "    for folder in folders:\n",
    "        for img_path in tqdm(os.listdir(folder)):\n",
    "            face = DeepFace.represent(img_path = f\"{folder}/{img_path}\", model_name = 'ArcFace', enforce_detection=False)\n",
    "            face.insert(0, folder)\n",
    "            faces.append(face)\n",
    "    \n",
    "    with open('FaceFeatures.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:11<00:00,  3.49it/s]\n",
      "100%|██████████| 57/57 [00:09<00:00,  5.75it/s]\n",
      "100%|██████████| 89/89 [00:38<00:00,  2.29it/s]\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "face_folders_path = [\"Ali_Daei\", \"Ali_Khamenei\",\"Asghar_Farhadi\",\"Bahare_Rahnama\"]\n",
    "gret_datad(face_folders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ali_Daei</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.660288</td>\n",
       "      <td>-0.158914</td>\n",
       "      <td>-0.432501</td>\n",
       "      <td>-0.028460</td>\n",
       "      <td>0.307886</td>\n",
       "      <td>-0.079241</td>\n",
       "      <td>-0.176694</td>\n",
       "      <td>0.054402</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048308</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.473464</td>\n",
       "      <td>-0.737842</td>\n",
       "      <td>0.319790</td>\n",
       "      <td>-0.388995</td>\n",
       "      <td>-0.035990</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>-3.948393e-01</td>\n",
       "      <td>-1.259605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ali_Daei</td>\n",
       "      <td>-0.002693</td>\n",
       "      <td>-0.119930</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>0.058870</td>\n",
       "      <td>-0.125557</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.094410</td>\n",
       "      <td>-0.009586</td>\n",
       "      <td>-0.174662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184983</td>\n",
       "      <td>-0.058797</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>0.052264</td>\n",
       "      <td>0.285303</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>-0.106605</td>\n",
       "      <td>-0.024422</td>\n",
       "      <td>6.333840e-02</td>\n",
       "      <td>-0.146956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ali_Daei</td>\n",
       "      <td>-0.044753</td>\n",
       "      <td>0.305361</td>\n",
       "      <td>-0.021155</td>\n",
       "      <td>-0.041216</td>\n",
       "      <td>-0.225892</td>\n",
       "      <td>0.275694</td>\n",
       "      <td>0.151403</td>\n",
       "      <td>-0.022977</td>\n",
       "      <td>-0.026036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401451</td>\n",
       "      <td>0.167239</td>\n",
       "      <td>0.018569</td>\n",
       "      <td>-0.038476</td>\n",
       "      <td>-0.001218</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>-0.089374</td>\n",
       "      <td>-0.023997</td>\n",
       "      <td>2.182834e-02</td>\n",
       "      <td>-0.179531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ali_Daei</td>\n",
       "      <td>-0.280349</td>\n",
       "      <td>0.565518</td>\n",
       "      <td>-0.203952</td>\n",
       "      <td>-0.456724</td>\n",
       "      <td>-0.022331</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>-0.143302</td>\n",
       "      <td>-0.103409</td>\n",
       "      <td>-0.017181</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.229651</td>\n",
       "      <td>0.242043</td>\n",
       "      <td>0.436608</td>\n",
       "      <td>-0.763917</td>\n",
       "      <td>0.435238</td>\n",
       "      <td>-0.292057</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>-4.746403e-01</td>\n",
       "      <td>-1.146207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ali_Daei</td>\n",
       "      <td>-0.322480</td>\n",
       "      <td>0.924192</td>\n",
       "      <td>-0.226081</td>\n",
       "      <td>-0.705730</td>\n",
       "      <td>-0.068783</td>\n",
       "      <td>0.462703</td>\n",
       "      <td>-0.261494</td>\n",
       "      <td>-0.132286</td>\n",
       "      <td>-0.016097</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.617778</td>\n",
       "      <td>0.360246</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>-1.054100</td>\n",
       "      <td>0.414754</td>\n",
       "      <td>-0.432683</td>\n",
       "      <td>-0.042040</td>\n",
       "      <td>0.045085</td>\n",
       "      <td>-4.932705e-01</td>\n",
       "      <td>-1.521108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Bahare_Rahnama</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>0.353906</td>\n",
       "      <td>-0.045549</td>\n",
       "      <td>-0.275057</td>\n",
       "      <td>0.133867</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>-0.049449</td>\n",
       "      <td>0.032951</td>\n",
       "      <td>0.201069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.576328</td>\n",
       "      <td>-0.153197</td>\n",
       "      <td>-0.067814</td>\n",
       "      <td>-0.135540</td>\n",
       "      <td>0.015273</td>\n",
       "      <td>0.027725</td>\n",
       "      <td>0.062524</td>\n",
       "      <td>6.247296e-02</td>\n",
       "      <td>0.147085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Bahare_Rahnama</td>\n",
       "      <td>-0.345089</td>\n",
       "      <td>0.897888</td>\n",
       "      <td>-0.271182</td>\n",
       "      <td>-0.580523</td>\n",
       "      <td>-0.085821</td>\n",
       "      <td>0.476722</td>\n",
       "      <td>-0.190755</td>\n",
       "      <td>-0.101353</td>\n",
       "      <td>-0.057764</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.489817</td>\n",
       "      <td>0.221170</td>\n",
       "      <td>0.693253</td>\n",
       "      <td>-1.049710</td>\n",
       "      <td>0.465724</td>\n",
       "      <td>-0.397086</td>\n",
       "      <td>-0.098636</td>\n",
       "      <td>0.084051</td>\n",
       "      <td>-5.404941e-01</td>\n",
       "      <td>-1.693052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Bahare_Rahnama</td>\n",
       "      <td>-0.303771</td>\n",
       "      <td>0.659261</td>\n",
       "      <td>-0.237314</td>\n",
       "      <td>-0.538999</td>\n",
       "      <td>-0.034628</td>\n",
       "      <td>0.494026</td>\n",
       "      <td>-0.169166</td>\n",
       "      <td>-0.081168</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.374039</td>\n",
       "      <td>0.280655</td>\n",
       "      <td>0.579324</td>\n",
       "      <td>-0.921438</td>\n",
       "      <td>0.476433</td>\n",
       "      <td>-0.356982</td>\n",
       "      <td>-0.102988</td>\n",
       "      <td>0.083187</td>\n",
       "      <td>-5.763821e-01</td>\n",
       "      <td>-1.416769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Bahare_Rahnama</td>\n",
       "      <td>0.052706</td>\n",
       "      <td>0.068925</td>\n",
       "      <td>-0.019370</td>\n",
       "      <td>0.023254</td>\n",
       "      <td>-0.175986</td>\n",
       "      <td>-0.128753</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>0.105680</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077960</td>\n",
       "      <td>0.266547</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>-0.172719</td>\n",
       "      <td>-0.210727</td>\n",
       "      <td>0.081229</td>\n",
       "      <td>0.298589</td>\n",
       "      <td>0.221437</td>\n",
       "      <td>5.597249e-07</td>\n",
       "      <td>0.019134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Bahare_Rahnama</td>\n",
       "      <td>-0.335922</td>\n",
       "      <td>0.603519</td>\n",
       "      <td>-0.164972</td>\n",
       "      <td>-0.676987</td>\n",
       "      <td>0.049577</td>\n",
       "      <td>0.476392</td>\n",
       "      <td>-0.241575</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>-0.089141</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.580610</td>\n",
       "      <td>0.272533</td>\n",
       "      <td>0.518322</td>\n",
       "      <td>-0.859409</td>\n",
       "      <td>0.293458</td>\n",
       "      <td>-0.387514</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.175383</td>\n",
       "      <td>-4.201306e-01</td>\n",
       "      <td>-1.710191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "0          Ali_Daei -0.286591  0.660288 -0.158914 -0.432501 -0.028460   \n",
       "1          Ali_Daei -0.002693 -0.119930 -0.072740  0.058870 -0.125557   \n",
       "2          Ali_Daei -0.044753  0.305361 -0.021155 -0.041216 -0.225892   \n",
       "3          Ali_Daei -0.280349  0.565518 -0.203952 -0.456724 -0.022331   \n",
       "4          Ali_Daei -0.322480  0.924192 -0.226081 -0.705730 -0.068783   \n",
       "..              ...       ...       ...       ...       ...       ...   \n",
       "175  Bahare_Rahnama  0.104194  0.353906 -0.045549 -0.275057  0.133867   \n",
       "176  Bahare_Rahnama -0.345089  0.897888 -0.271182 -0.580523 -0.085821   \n",
       "177  Bahare_Rahnama -0.303771  0.659261 -0.237314 -0.538999 -0.034628   \n",
       "178  Bahare_Rahnama  0.052706  0.068925 -0.019370  0.023254 -0.175986   \n",
       "179  Bahare_Rahnama -0.335922  0.603519 -0.164972 -0.676987  0.049577   \n",
       "\n",
       "          6         7         8         9    ...       503       504  \\\n",
       "0    0.307886 -0.079241 -0.176694  0.054402  ... -1.048308  0.117746   \n",
       "1    0.021472  0.094410 -0.009586 -0.174662  ... -0.184983 -0.058797   \n",
       "2    0.275694  0.151403 -0.022977 -0.026036  ... -0.401451  0.167239   \n",
       "3    0.377768 -0.143302 -0.103409 -0.017181  ... -1.229651  0.242043   \n",
       "4    0.462703 -0.261494 -0.132286 -0.016097  ... -1.617778  0.360246   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "175  0.072980 -0.049449  0.032951  0.201069  ...  0.054794  0.576328   \n",
       "176  0.476722 -0.190755 -0.101353 -0.057764  ... -1.489817  0.221170   \n",
       "177  0.494026 -0.169166 -0.081168 -0.009632  ... -1.374039  0.280655   \n",
       "178 -0.128753  0.036056  0.105680  0.025595  ...  0.077960  0.266547   \n",
       "179  0.476392 -0.241575  0.001663 -0.089141  ... -1.580610  0.272533   \n",
       "\n",
       "          505       506       507       508       509       510           511  \\\n",
       "0    0.473464 -0.737842  0.319790 -0.388995 -0.035990  0.000837 -3.948393e-01   \n",
       "1    0.032070  0.052264  0.285303  0.005907 -0.106605 -0.024422  6.333840e-02   \n",
       "2    0.018569 -0.038476 -0.001218  0.045305 -0.089374 -0.023997  2.182834e-02   \n",
       "3    0.436608 -0.763917  0.435238 -0.292057 -0.021628  0.058926 -4.746403e-01   \n",
       "4    0.522500 -1.054100  0.414754 -0.432683 -0.042040  0.045085 -4.932705e-01   \n",
       "..        ...       ...       ...       ...       ...       ...           ...   \n",
       "175 -0.153197 -0.067814 -0.135540  0.015273  0.027725  0.062524  6.247296e-02   \n",
       "176  0.693253 -1.049710  0.465724 -0.397086 -0.098636  0.084051 -5.404941e-01   \n",
       "177  0.579324 -0.921438  0.476433 -0.356982 -0.102988  0.083187 -5.763821e-01   \n",
       "178 -0.010719 -0.172719 -0.210727  0.081229  0.298589  0.221437  5.597249e-07   \n",
       "179  0.518322 -0.859409  0.293458 -0.387514  0.007563  0.175383 -4.201306e-01   \n",
       "\n",
       "          512  \n",
       "0   -1.259605  \n",
       "1   -0.146956  \n",
       "2   -0.179531  \n",
       "3   -1.146207  \n",
       "4   -1.521108  \n",
       "..        ...  \n",
       "175  0.147085  \n",
       "176 -1.693052  \n",
       "177 -1.416769  \n",
       "178  0.019134  \n",
       "179 -1.710191  \n",
       "\n",
       "[180 rows x 513 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"FaceFeatures.csv\", header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.660288</td>\n",
       "      <td>-0.158914</td>\n",
       "      <td>-0.432501</td>\n",
       "      <td>-0.028460</td>\n",
       "      <td>0.307886</td>\n",
       "      <td>-0.079241</td>\n",
       "      <td>-0.176694</td>\n",
       "      <td>0.054402</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048308</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.473464</td>\n",
       "      <td>-0.737842</td>\n",
       "      <td>0.319790</td>\n",
       "      <td>-0.388995</td>\n",
       "      <td>-0.035990</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>-3.948393e-01</td>\n",
       "      <td>-1.259605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.002693</td>\n",
       "      <td>-0.119930</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>0.058870</td>\n",
       "      <td>-0.125557</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.094410</td>\n",
       "      <td>-0.009586</td>\n",
       "      <td>-0.174662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184983</td>\n",
       "      <td>-0.058797</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>0.052264</td>\n",
       "      <td>0.285303</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>-0.106605</td>\n",
       "      <td>-0.024422</td>\n",
       "      <td>6.333840e-02</td>\n",
       "      <td>-0.146956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.044753</td>\n",
       "      <td>0.305361</td>\n",
       "      <td>-0.021155</td>\n",
       "      <td>-0.041216</td>\n",
       "      <td>-0.225892</td>\n",
       "      <td>0.275694</td>\n",
       "      <td>0.151403</td>\n",
       "      <td>-0.022977</td>\n",
       "      <td>-0.026036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401451</td>\n",
       "      <td>0.167239</td>\n",
       "      <td>0.018569</td>\n",
       "      <td>-0.038476</td>\n",
       "      <td>-0.001218</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>-0.089374</td>\n",
       "      <td>-0.023997</td>\n",
       "      <td>2.182834e-02</td>\n",
       "      <td>-0.179531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.280349</td>\n",
       "      <td>0.565518</td>\n",
       "      <td>-0.203952</td>\n",
       "      <td>-0.456724</td>\n",
       "      <td>-0.022331</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>-0.143302</td>\n",
       "      <td>-0.103409</td>\n",
       "      <td>-0.017181</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.229651</td>\n",
       "      <td>0.242043</td>\n",
       "      <td>0.436608</td>\n",
       "      <td>-0.763917</td>\n",
       "      <td>0.435238</td>\n",
       "      <td>-0.292057</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>-4.746403e-01</td>\n",
       "      <td>-1.146207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.322480</td>\n",
       "      <td>0.924192</td>\n",
       "      <td>-0.226081</td>\n",
       "      <td>-0.705730</td>\n",
       "      <td>-0.068783</td>\n",
       "      <td>0.462703</td>\n",
       "      <td>-0.261494</td>\n",
       "      <td>-0.132286</td>\n",
       "      <td>-0.016097</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.617778</td>\n",
       "      <td>0.360246</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>-1.054100</td>\n",
       "      <td>0.414754</td>\n",
       "      <td>-0.432683</td>\n",
       "      <td>-0.042040</td>\n",
       "      <td>0.045085</td>\n",
       "      <td>-4.932705e-01</td>\n",
       "      <td>-1.521108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>0.353906</td>\n",
       "      <td>-0.045549</td>\n",
       "      <td>-0.275057</td>\n",
       "      <td>0.133867</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>-0.049449</td>\n",
       "      <td>0.032951</td>\n",
       "      <td>0.201069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.576328</td>\n",
       "      <td>-0.153197</td>\n",
       "      <td>-0.067814</td>\n",
       "      <td>-0.135540</td>\n",
       "      <td>0.015273</td>\n",
       "      <td>0.027725</td>\n",
       "      <td>0.062524</td>\n",
       "      <td>6.247296e-02</td>\n",
       "      <td>0.147085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.345089</td>\n",
       "      <td>0.897888</td>\n",
       "      <td>-0.271182</td>\n",
       "      <td>-0.580523</td>\n",
       "      <td>-0.085821</td>\n",
       "      <td>0.476722</td>\n",
       "      <td>-0.190755</td>\n",
       "      <td>-0.101353</td>\n",
       "      <td>-0.057764</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.489817</td>\n",
       "      <td>0.221170</td>\n",
       "      <td>0.693253</td>\n",
       "      <td>-1.049710</td>\n",
       "      <td>0.465724</td>\n",
       "      <td>-0.397086</td>\n",
       "      <td>-0.098636</td>\n",
       "      <td>0.084051</td>\n",
       "      <td>-5.404941e-01</td>\n",
       "      <td>-1.693052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.303771</td>\n",
       "      <td>0.659261</td>\n",
       "      <td>-0.237314</td>\n",
       "      <td>-0.538999</td>\n",
       "      <td>-0.034628</td>\n",
       "      <td>0.494026</td>\n",
       "      <td>-0.169166</td>\n",
       "      <td>-0.081168</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.374039</td>\n",
       "      <td>0.280655</td>\n",
       "      <td>0.579324</td>\n",
       "      <td>-0.921438</td>\n",
       "      <td>0.476433</td>\n",
       "      <td>-0.356982</td>\n",
       "      <td>-0.102988</td>\n",
       "      <td>0.083187</td>\n",
       "      <td>-5.763821e-01</td>\n",
       "      <td>-1.416769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3</td>\n",
       "      <td>0.052706</td>\n",
       "      <td>0.068925</td>\n",
       "      <td>-0.019370</td>\n",
       "      <td>0.023254</td>\n",
       "      <td>-0.175986</td>\n",
       "      <td>-0.128753</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>0.105680</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077960</td>\n",
       "      <td>0.266547</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>-0.172719</td>\n",
       "      <td>-0.210727</td>\n",
       "      <td>0.081229</td>\n",
       "      <td>0.298589</td>\n",
       "      <td>0.221437</td>\n",
       "      <td>5.597249e-07</td>\n",
       "      <td>0.019134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.335922</td>\n",
       "      <td>0.603519</td>\n",
       "      <td>-0.164972</td>\n",
       "      <td>-0.676987</td>\n",
       "      <td>0.049577</td>\n",
       "      <td>0.476392</td>\n",
       "      <td>-0.241575</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>-0.089141</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.580610</td>\n",
       "      <td>0.272533</td>\n",
       "      <td>0.518322</td>\n",
       "      <td>-0.859409</td>\n",
       "      <td>0.293458</td>\n",
       "      <td>-0.387514</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.175383</td>\n",
       "      <td>-4.201306e-01</td>\n",
       "      <td>-1.710191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6    \\\n",
       "0      0 -0.286591  0.660288 -0.158914 -0.432501 -0.028460  0.307886   \n",
       "1      0 -0.002693 -0.119930 -0.072740  0.058870 -0.125557  0.021472   \n",
       "2      0 -0.044753  0.305361 -0.021155 -0.041216 -0.225892  0.275694   \n",
       "3      0 -0.280349  0.565518 -0.203952 -0.456724 -0.022331  0.377768   \n",
       "4      0 -0.322480  0.924192 -0.226081 -0.705730 -0.068783  0.462703   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "175    3  0.104194  0.353906 -0.045549 -0.275057  0.133867  0.072980   \n",
       "176    3 -0.345089  0.897888 -0.271182 -0.580523 -0.085821  0.476722   \n",
       "177    3 -0.303771  0.659261 -0.237314 -0.538999 -0.034628  0.494026   \n",
       "178    3  0.052706  0.068925 -0.019370  0.023254 -0.175986 -0.128753   \n",
       "179    3 -0.335922  0.603519 -0.164972 -0.676987  0.049577  0.476392   \n",
       "\n",
       "          7         8         9    ...       503       504       505  \\\n",
       "0   -0.079241 -0.176694  0.054402  ... -1.048308  0.117746  0.473464   \n",
       "1    0.094410 -0.009586 -0.174662  ... -0.184983 -0.058797  0.032070   \n",
       "2    0.151403 -0.022977 -0.026036  ... -0.401451  0.167239  0.018569   \n",
       "3   -0.143302 -0.103409 -0.017181  ... -1.229651  0.242043  0.436608   \n",
       "4   -0.261494 -0.132286 -0.016097  ... -1.617778  0.360246  0.522500   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "175 -0.049449  0.032951  0.201069  ...  0.054794  0.576328 -0.153197   \n",
       "176 -0.190755 -0.101353 -0.057764  ... -1.489817  0.221170  0.693253   \n",
       "177 -0.169166 -0.081168 -0.009632  ... -1.374039  0.280655  0.579324   \n",
       "178  0.036056  0.105680  0.025595  ...  0.077960  0.266547 -0.010719   \n",
       "179 -0.241575  0.001663 -0.089141  ... -1.580610  0.272533  0.518322   \n",
       "\n",
       "          506       507       508       509       510           511       512  \n",
       "0   -0.737842  0.319790 -0.388995 -0.035990  0.000837 -3.948393e-01 -1.259605  \n",
       "1    0.052264  0.285303  0.005907 -0.106605 -0.024422  6.333840e-02 -0.146956  \n",
       "2   -0.038476 -0.001218  0.045305 -0.089374 -0.023997  2.182834e-02 -0.179531  \n",
       "3   -0.763917  0.435238 -0.292057 -0.021628  0.058926 -4.746403e-01 -1.146207  \n",
       "4   -1.054100  0.414754 -0.432683 -0.042040  0.045085 -4.932705e-01 -1.521108  \n",
       "..        ...       ...       ...       ...       ...           ...       ...  \n",
       "175 -0.067814 -0.135540  0.015273  0.027725  0.062524  6.247296e-02  0.147085  \n",
       "176 -1.049710  0.465724 -0.397086 -0.098636  0.084051 -5.404941e-01 -1.693052  \n",
       "177 -0.921438  0.476433 -0.356982 -0.102988  0.083187 -5.763821e-01 -1.416769  \n",
       "178 -0.172719 -0.210727  0.081229  0.298589  0.221437  5.597249e-07  0.019134  \n",
       "179 -0.859409  0.293458 -0.387514  0.007563  0.175383 -4.201306e-01 -1.710191  \n",
       "\n",
       "[180 rows x 513 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace([\"Ali_Daei\", \"Ali_Khamenei\",\"Asghar_Farhadi\",\"Bahare_Rahnama\"],\n",
    "                    [0, 1, 2, 3])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:, 1:].values\n",
    "X.shape\n",
    "Y = data.iloc[:, 0].values\n",
    "Y = Y.reshape(-1, 1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_Test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Dense(512,  activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(4, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 11ms/step - loss: 1.3558 - accuracy: 0.3125\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.2845 - accuracy: 0.5208\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2350 - accuracy: 0.6111\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1955 - accuracy: 0.6111\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1450 - accuracy: 0.6806\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1009 - accuracy: 0.7292\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.0493 - accuracy: 0.7847\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.9977 - accuracy: 0.7639\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9480 - accuracy: 0.7569\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8961 - accuracy: 0.7569\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.8466 - accuracy: 0.7708\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8000 - accuracy: 0.7986\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7578 - accuracy: 0.7917\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7140 - accuracy: 0.7986\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6773 - accuracy: 0.7986\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6467 - accuracy: 0.8264\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6108 - accuracy: 0.8333\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5870 - accuracy: 0.8264\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5610 - accuracy: 0.8403\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5450 - accuracy: 0.8264\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5130 - accuracy: 0.8542\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5069 - accuracy: 0.8264\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4975 - accuracy: 0.8264\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4732 - accuracy: 0.8403\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4533 - accuracy: 0.8542\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4378 - accuracy: 0.8889\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4480 - accuracy: 0.8403\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4173 - accuracy: 0.8681\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4010 - accuracy: 0.8819\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3975 - accuracy: 0.8542\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3898 - accuracy: 0.8681\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3725 - accuracy: 0.8403\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3581 - accuracy: 0.8819\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3542 - accuracy: 0.8681\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3548 - accuracy: 0.8889\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3316 - accuracy: 0.8958\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3164 - accuracy: 0.8889\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3150 - accuracy: 0.8958\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3296 - accuracy: 0.8611\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2972 - accuracy: 0.8958\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2842 - accuracy: 0.9097\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2964 - accuracy: 0.8681\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3004 - accuracy: 0.8750\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2732 - accuracy: 0.9306\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2786 - accuracy: 0.9028\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2799 - accuracy: 0.8819\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2579 - accuracy: 0.9097\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2615 - accuracy: 0.8958\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2502 - accuracy: 0.9097\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2448 - accuracy: 0.9306\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2402 - accuracy: 0.8958\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2181 - accuracy: 0.9375\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2178 - accuracy: 0.9236\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2263 - accuracy: 0.9097\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2302 - accuracy: 0.9097\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2036 - accuracy: 0.9444\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2035 - accuracy: 0.9306\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2416 - accuracy: 0.8889\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1879 - accuracy: 0.9236\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2142 - accuracy: 0.9375\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2094 - accuracy: 0.9236\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1875 - accuracy: 0.9514\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1854 - accuracy: 0.9514\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1656 - accuracy: 0.9722\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1695 - accuracy: 0.9514\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1611 - accuracy: 0.9514\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1599 - accuracy: 0.9722\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.9653\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1513 - accuracy: 0.9653\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1563 - accuracy: 0.9583\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1448 - accuracy: 0.9722\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1396 - accuracy: 0.9653\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1377 - accuracy: 0.9792\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1391 - accuracy: 0.9792\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1525 - accuracy: 0.9583\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1647 - accuracy: 0.9375\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1694 - accuracy: 0.9514\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1265 - accuracy: 0.9861\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1372 - accuracy: 0.9792\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1312 - accuracy: 0.9653\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1236 - accuracy: 0.9792\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1268 - accuracy: 0.9722\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1084 - accuracy: 0.9861\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1175 - accuracy: 0.9722\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1070 - accuracy: 0.9861\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1026 - accuracy: 0.9792\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0973 - accuracy: 0.9861\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1009 - accuracy: 0.9792\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0933 - accuracy: 0.9792\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0943 - accuracy: 0.9792\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1075 - accuracy: 0.9653\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1061 - accuracy: 0.9792\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0906 - accuracy: 0.9861\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0858 - accuracy: 0.9792\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0922 - accuracy: 0.9792\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0848 - accuracy: 0.9861\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0793 - accuracy: 0.9792\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0786 - accuracy: 0.9861\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0866 - accuracy: 0.9861\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0829 - accuracy: 0.9861\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0758 - accuracy: 0.9931\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0755 - accuracy: 0.9861\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 0.9861\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0679 - accuracy: 0.9931\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 0.9861\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0635 - accuracy: 0.9861\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0648 - accuracy: 0.9931\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0664 - accuracy: 0.9861\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0602 - accuracy: 0.9861\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0596 - accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0576 - accuracy: 0.9931\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 0.9861\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0562 - accuracy: 0.9931\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0533 - accuracy: 0.9931\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.9861\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0518 - accuracy: 0.9931\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0568 - accuracy: 0.9931\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0541 - accuracy: 0.9931\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0480 - accuracy: 0.9931\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0605 - accuracy: 0.9931\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0489 - accuracy: 0.9931\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0440 - accuracy: 0.9931\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0501 - accuracy: 0.9931\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0462 - accuracy: 0.9931\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0459 - accuracy: 0.9931\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0433 - accuracy: 0.9861\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0497 - accuracy: 0.9861\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0539 - accuracy: 0.9931\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0523 - accuracy: 0.9931\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0731 - accuracy: 0.9722\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0419 - accuracy: 0.9931\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0324 - accuracy: 0.9931\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0332 - accuracy: 0.9931\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 0.9931\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0310 - accuracy: 0.9931\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0297 - accuracy: 0.9931\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0265 - accuracy: 0.9931\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0279 - accuracy: 0.9931\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0316 - accuracy: 0.9931\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9792\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0370 - accuracy: 0.9861\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0299 - accuracy: 0.9931\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0461 - accuracy: 0.9861\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0384 - accuracy: 0.9861\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0280 - accuracy: 0.9931\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0250 - accuracy: 0.9931\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "MLP_output = model.fit(X_train, Y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"weights/face_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.8611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4986480176448822, 0.8611111044883728]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_Test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e3d5a221850d49c091bfa422729480c31cc2214df6b2f2f1ac4431d31ee6b32"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
